{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Création d'un Dataset à partir des images scrapées de ZARA. Le dateset est sauvegardé dans le dossier DataSets du répoertoire du projet.\n",
    "Le Dataset se présente sous la forme d'un dictionnaire contenant les données et les labels\n",
    "sous différents formats ainsi que des caractéristiques  (global features, hog features) extraites des images \n",
    "\n",
    "Le répertoire de travail se présente sous la forme suivante : \n",
    "\n",
    "Projet_Classification_ZARA (MAINDIR) \n",
    "    Images (IMAGEDIR)\n",
    "        jupes (category)\n",
    "        pantalons\n",
    "        robes\n",
    "        t-shirts\n",
    "    DataSets (DATASETDIR)\n",
    "       \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Package\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# standard packages\n",
    "## https://docs.python.org/3/library/io.html\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image processing and features extraction packages\n",
    "## https://python-guide-pt-br.readthedocs.io/fr/latest/scenarios/imaging.html\n",
    "## https://docs.python.org/fr/3/library/hashlib.html\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import mahotas\n",
    "import cv2\n",
    "import h5py\n",
    "import hashlib # permet de chiffrer \n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "from skimage import color\n",
    "from skimage.io import imread\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "# Dataset creation packages\n",
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition du répertoire de Travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## work directories ##\n",
    "######################\n",
    "\n",
    "MAINDIR='C:/Users/utilisateur/Documents/DataIA/Projet/Projet_Classification_ZARA/' # répertoire du projet\n",
    "IMAGEDIR= MAINDIR + 'Images/'+'Images_models/' # répertoire contenant les images réparties  dans les dossiers categories\n",
    "CATEGORIES=os.listdir(IMAGEDIR) #ie : jupes, pantalons, robes, t-shirts (attention j'ai un s à la fin des termes)\n",
    "DATASETDIR=MAINDIR + 'DataSets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renommer les images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "## FONCTION POUR RENOMMER LES IMAGES en \"category\"_xx.jpg ##\n",
    "############################################################\n",
    "def rename_file(CATEGORIES):\n",
    "    \"\"\" Rename image files, in category folder, in the form \"category\"_xx.jpg ; ie: jupe_0.jpg, jupe_1.jpg etc... \"\"\"\n",
    "    for category in CATEGORIES:\n",
    "        path=os.path.join(IMAGEDIR,category)\n",
    "        for i, filename in enumerate(os.listdir(path)):\n",
    "            os.rename(path + \"/\" + filename, path + \"/\" + category[:-1]+'_'+ str(i) + \".jpg\") # folder[:-1] enlever le s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Extraction  des features et traitement des images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## Features Extraction functions ##\n",
    "###################################\n",
    "\n",
    "## Fixed Variables ##\n",
    "fixed_size = tuple((56, 56))  # size for resizing \n",
    "bins=8\n",
    "\n",
    "# feature-descriptor-1 : Hu-Moments that quantifies shape \n",
    "def fd_hu_moments(image):\n",
    "    # convert the image to grayscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # compute the Hu moment feature vector\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature\n",
    "\n",
    "\n",
    "# feature-descriptor-2: Haralick Texture\n",
    "def fd_haralick(image):\n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # compute the haralick texture feature vector\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    # return the result\n",
    "    return haralick\n",
    "\n",
    "# feature-descriptor-3: Color Histogram\n",
    "def fd_histogram(image, mask=None):\n",
    "    # convert the image to HSV color-space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    # compute the color histogram\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    # normalize the histogram\n",
    "    cv2.normalize(hist, hist)\n",
    "    # return the histogram\n",
    "    return hist.flatten()\n",
    "\n",
    "# histogramme of oriented gradient \n",
    "def fd_Hog(image):\n",
    "    # convert the image to grayscale\n",
    "    image = color.rgb2gray(image)\n",
    "    # compute hog \n",
    "    cell_size = (16, 16)  # h x w in pixels\n",
    "    block_size = (4, 4)  # h x w in cells\n",
    "    nbins = 9  # number of orientation bins\n",
    "\n",
    "    # winSize is the size of the image cropped to an multiple of the cell size\n",
    "    # cell_size is the size of the cells of the img patch over which to calculate the histograms\n",
    "    # block_size is the number of cells which fit in the patch\n",
    "\n",
    "    fd,hog_image = hog(image, orientations=8, pixels_per_cell=cell_size,cells_per_block=block_size,block_norm='L2-Hys',visualise=True)\n",
    "    return fd,hog_image\n",
    "\n",
    "\n",
    "# Put all those features-descriptor into a function\n",
    "def Global_feature_extraction(resized_image):\n",
    "    ####################################\n",
    "    # Global Feature extraction\n",
    "    ####################################\n",
    "    fv_hu_moments = fd_hu_moments(resized_image)\n",
    "    fv_haralick   = fd_haralick(resized_image)\n",
    "    fv_histogram  = fd_histogram(resized_image)\n",
    "\n",
    "    ###################################\n",
    "    # Concatenate global features\n",
    "    ###################################\n",
    "    global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "    return global_feature\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "## Image treatment : resizing and grayscale ##\n",
    "##############################################\n",
    "def traitement_image(image_path):\n",
    "    ### Load image ###\n",
    "    img = cv2.imread(image_path)\n",
    "#   plt.imshow(img)\n",
    "#   plt.show()\n",
    "\n",
    "    ### resize ###\n",
    "    resized=cv2.resize(img, fixed_size, interpolation=cv2.INTER_AREA)\n",
    "#   plt.imshow(resized)\n",
    "#   plt.show()\n",
    "\n",
    "    ### convert the image to grayscale ###\n",
    "    imgGray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)  \n",
    "#   cv2.imshow('title',imgGray)\n",
    "#   cv2.waitKey(0)\n",
    "#   cv2.destroyAllWindows()\n",
    "    return imgGray,resized\n",
    "\n",
    "\n",
    "###################################################################\n",
    "##  dress segmentation : traitement pour  masquer les mannequins ##\n",
    "###################################################################\n",
    "\n",
    "def traitement_image_mask(image_path):\n",
    "    #img= cv2.imread(img)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img,(224,224))\n",
    "        \n",
    "    mask = np.zeros(img.shape[:2],np.uint8)\n",
    "    bgdModel = np.zeros((1,65),np.float64)\n",
    "    fgdModel = np.zeros((1,65),np.float64)\n",
    "    height, width = img.shape[:2]\n",
    "    #rect = (50,60,width-100,height-60) ### pantalons\n",
    "    rect = (50,50,width-100,height-60) #### robes\n",
    "    #rect = (20,20,width-40,height-20) #### jupes\n",
    "    cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "    img2 = img*mask2[:,:,np.newaxis]\n",
    "    img2[mask2 == 0] = (255, 255, 255)\n",
    "    \n",
    "    final = np.ones(img.shape,np.uint8)*0 + img2\n",
    "    \n",
    "    return cv2.cvtColor(final, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création  et sauvegarde du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "## CREATION D'un Dataset à partir des images traitées ##\n",
    "########################################################\n",
    "# empty lists\n",
    "Data = [] # Data contient la matrice des pixels de l'image en ligne => 1 lignes = 1 images (par ex Image[0] : 1x(3136) si on fait resize 56par56 )\n",
    "Target_name = [] # Target_Name C'est la classe de chaque image sous forme ('jupe', 'robe', 't-shirt', 'pantalon')\n",
    "Data_name = [] # Data_name contient le nom de l'image  par exemple : jupe_01\n",
    "Image=[] #Image est une matrice de matrices => 1 ligne = 1 images en format matrice (par ex  Image[0] : 1x56x56)\n",
    "Global_features=[]\n",
    "Dataset_dic={} # Le DataSet sous fome de dictionnaire qui contiendra data, images, target etc etc \n",
    "Hog_images = [] # hog images\n",
    "Hog_features = [] # hog features\n",
    "\n",
    "def DataSet():\n",
    "    for category in CATEGORIES: \n",
    "        clothe_class=category[:-1]  #ie jupe, pantalon, robe or t-shirt\n",
    "        for image in os.listdir(IMAGEDIR+category):  #image : nom de l'image ie jupe_0.jpg\n",
    "            image_path =os.path.join(IMAGEDIR+category+'/'+image) \n",
    "            # image treatment \n",
    "            imgGray,resized=traitement_image(image_path)\n",
    "#           # matrix to line with flatten()\n",
    "            imgGray_flat=np.array(imgGray).flatten()\n",
    "#             # global feature extraction\n",
    "            global_feature=Global_feature_extraction(resized)\n",
    "            # Hog\n",
    "            \n",
    "            img_hog=imread(image_path)\n",
    "            img_hog = resize(img_hog, (200,200))\n",
    "            fd,hog_image=fd_Hog(img_hog)\n",
    "            # update the lists\n",
    "            try:\n",
    "                Image.append(imgGray)\n",
    "                Target_name.append(clothe_class)\n",
    "                Data.append(imgGray_flat)\n",
    "                Data_name.append(image[:-4])\n",
    "                Global_features.append(global_feature)\n",
    "                Hog_images.append(hog_image)\n",
    "                Hog_features.append(fd)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(Data),np.array(Image), np.array(Target_name),np.array(Data_name), np.array(Hog_features), np.array(Hog_images), np.array(Global_features)\n",
    "\n",
    "def DataSetCreation(name:str):\n",
    "    ## on crée le data set\n",
    "    Data, Image, Target_name, Data_Name ,Hog_features,Hog_images, Global_features = DataSet()  \n",
    "    \n",
    "    ## vérification des dimensions \n",
    "    print(\"[STATUS] Data (pixel matrix in line) size {}\".format(Data.shape) )\n",
    "    print(\"[STATUS] Target name (Labels) size {}\".format(Target_name.shape) )\n",
    "    print(\"[STATUS] Data Name (Image names) size {}\".format(Data_Name.shape) )\n",
    "    print(\"[STATUS] Image  (Image matrix) size {}\".format(Image.shape) )\n",
    "    print(\"[STATUS] global features (Image names) size {}\".format(Global_features.shape) )\n",
    "    print(\"[STATUS] Hog features  size {}\".format(np.array(Hog_features).shape) )\n",
    "    print(\"[STATUS] Hog images  size {}\".format(Hog_images.shape) )\n",
    "    \n",
    "    ### mélange des données avec shuffle ; unison c'est pour que tous les arrays soient mélanger de la même façon et q'uon garde la correspondace entre eux\n",
    "    Data, Image,Target_name, Data_Name, Hog_features,Hog_images, Global_features = shuffle_arrays_unison(arrays=[Data, Image,Target_name, Data_Name, Hog_features,Hog_images,Global_features], random_seed=0)\n",
    "   \n",
    "    ### encodage  pantalon to 1 ; jupe to 0 ;  t-shirt to 3 and robe to 2 pour chaque images \n",
    "    \n",
    "    Target=pd.Series(Target_name).astype('category').cat.codes\n",
    "    #print(Target)\n",
    "    \n",
    "    #### création d'un dictionnaire pour savoir la correspondance code <-> vetement \n",
    "    Target_name_list = dict(enumerate(pd.Series(Target_name).astype('category').cat.categories)) # ie {0: 'jupe', 1: 'pantalon', 2: 'robe', 3: 't-shirt'}\n",
    "    print(\"[STATUS] Correspondance Class <-> Code {}\".format(Target_name_list) )\n",
    "    ### Creation du data set sous forme d'un dictionnaire comme quand on load les dataset scikitlearn\n",
    "    Dataset_dic={\n",
    "                        'data': Data,\n",
    "                        'target':Target,\n",
    "                        'target_names':Target_name,\n",
    "                        'target_name_list':list(Target_name_list.values()),\n",
    "                        'target_name_code': Target_name_list,\n",
    "                        'data_Name':Data_Name,\n",
    "                        'images': Image,\n",
    "                         'global_features':Global_features,\n",
    "                         'hog_features': Hog_features,\n",
    "                         'hog_images': Hog_images\n",
    "                       }\n",
    "    \n",
    "    ## pour sauver le dataset  (dans le dossier DataSets) et pouvoir le reloader plus tard dans un autre notebook\n",
    "    if not os.path.exists(DATASETDIR):\n",
    "        os.makedirs(DATASETDIR)\n",
    "    with open(DATASETDIR+'/'+ name +'.pickle',\"wb\") as outpout:\n",
    "        pickle.dump(Dataset_dic, outpout)\n",
    "    \n",
    "    return Dataset_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:239: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] Data (pixel matrix in line) size (1307, 3136)\n",
      "[STATUS] Target name (Labels) size (1307,)\n",
      "[STATUS] Data Name (Image names) size (1307,)\n",
      "[STATUS] Image  (Image matrix) size (1307, 56, 56)\n",
      "[STATUS] global features (Image names) size (1307, 532)\n",
      "[STATUS] Hog features  size (1307, 10368)\n",
      "[STATUS] Hog images  size (1307, 200, 200)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'shuffle_arrays_unison' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-187274b903ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Création du datase nommé 'ZARA_DataSet_models'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mZARA_DataSet_models\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataSetCreation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ZARA_DataSet_models_git'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-5f0356feee99>\u001b[0m in \u001b[0;36mDataSetCreation\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m### mélange des données avec shuffle ; unison c'est pour que tous les arrays soient mélanger de la même façon et q'uon garde la correspondace entre eux\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTarget_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mData_Name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHog_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHog_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobal_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle_arrays_unison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTarget_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mData_Name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHog_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHog_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGlobal_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m### encodage  pantalon to 1 ; jupe to 0 ;  t-shirt to 3 and robe to 2 pour chaque images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shuffle_arrays_unison' is not defined"
     ]
    }
   ],
   "source": [
    "## Création du datase nommé 'ZARA_DataSet_models'\n",
    "ZARA_DataSet_models=DataSetCreation('ZARA_DataSet_models_git')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vérification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérification du contenu du dataset\n",
    "print(ZARA_DataSet_models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des keys du dictionnaire dataset en variables globales\n",
    "for key in ZARA_DataSet_models.keys():\n",
    "    globals()[str(key)] =ZARA_DataSet_no_models[key]\n",
    "    \n",
    "# affichage de quelques images du dataset \n",
    "for i in range(0,6):\n",
    "    plt.imshow(images[i])\n",
    "    plt.imshow(hog_image[i], cmap='gray')\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pour charger le dataset dans un autre notebook\n",
    "# with open(DATASETDIR+'ZARA_DataSet_models.pickle', 'rb') as data:\n",
    "#     ZARA_DataSet_models = pickle.load(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZARA_DataSet_models_with_HOG.keys()\n",
    "for key in ZARA_DataSet_models_with_HOG.keys():\n",
    "    globals()[str(key)] =ZARA_DataSet_models_with_HOG[key]\n",
    "    \n",
    "for i in range(0,6):\n",
    "    print(data_Name[i])\n",
    "    print(target_names[i])\n",
    "    \n",
    "    plt.imshow(images[i])\n",
    "    plt.show()\n",
    "    plt.imshow(hog_images[i])\n",
    "    plt.show()\n",
    "    print('--------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
